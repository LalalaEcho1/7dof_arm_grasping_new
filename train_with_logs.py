#!/usr/bin/env python3
"""
å¸¦è¯¦ç»†æ—¥å¿—çš„è®­ç»ƒè„šæœ¬
"""

import os
import yaml
import numpy as np
from stable_baselines3 import SAC
from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback, BaseCallback
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize
from envs.seven_dof_arm import SevenDOFArmEnv

class TrainingLogger(BaseCallback):
    """è®­ç»ƒæ—¥å¿—è®°å½•å™¨"""
    
    def __init__(self, verbose=0):
        super(TrainingLogger, self).__init__(verbose)
        self.episode_rewards = []
        self.episode_lengths = []
        self.success_count = 0
        self.total_episodes = 0
        self.step_count = 0
        
    def _on_step(self) -> bool:
        self.step_count += 1
        
        # è®°å½•æ¯ä¸ªepisodeçš„å¥–åŠ±
        if len(self.locals.get('infos', [])) > 0:
            info = self.locals['infos'][0]
            if 'episode' in info:
                episode_reward = info['episode']['r']
                episode_length = info['episode']['l']
                self.episode_rewards.append(episode_reward)
                self.episode_lengths.append(episode_length)
                self.total_episodes += 1
                
                # æ£€æŸ¥æ˜¯å¦æˆåŠŸ
                if info.get('is_success', False):
                    self.success_count += 1
                
                # æ‰“å°episodeä¿¡æ¯
                success_rate = self.success_count / max(1, self.total_episodes)
                print(f"Episode {self.total_episodes}: Reward={episode_reward:.2f}, Length={episode_length}, Success={info.get('is_success', False)}, Success Rate={success_rate:.2%}")
                
                # æ¯10ä¸ªepisodeæ‰“å°ç»Ÿè®¡ä¿¡æ¯
                if self.total_episodes % 10 == 0:
                    avg_reward = np.mean(self.episode_rewards[-10:])
                    avg_length = np.mean(self.episode_lengths[-10:])
                    recent_success_rate = np.mean([1 if info.get('is_success', False) else 0 for info in self.locals.get('infos', [])[-10:]])
                    print(f"--- æœ€è¿‘10ä¸ªepisodesç»Ÿè®¡ ---")
                    print(f"å¹³å‡å¥–åŠ±: {avg_reward:.2f}")
                    print(f"å¹³å‡é•¿åº¦: {avg_length:.1f}")
                    print(f"æˆåŠŸç‡: {recent_success_rate:.2%}")
                    print(f"æ€»æ­¥æ•°: {self.step_count}")
                    print("-" * 40)
        
        return True

def make_env(render_mode=None):
    """å°è£…ç¯å¢ƒåˆ›å»º"""
    def _init():
        env = SevenDOFArmEnv(render_mode=render_mode, model_path='franka/panda.xml')
        env = Monitor(env)
        return env
    return _init

def train_with_logs():
    """å¸¦è¯¦ç»†æ—¥å¿—çš„è®­ç»ƒ"""
    print("ğŸš€ å¼€å§‹å¸¦æ—¥å¿—è®­ç»ƒ...")
    
    # åŠ è½½é…ç½®
    try:
        with open('config.yaml') as f:
            config = yaml.safe_load(f)
    except FileNotFoundError:
        print("é”™è¯¯ï¼šæ‰¾ä¸åˆ°config.yamlæ–‡ä»¶")
        return
    except yaml.YAMLError as e:
        print(f"é”™è¯¯ï¼šé…ç½®æ–‡ä»¶æ ¼å¼é”™è¯¯ - {e}")
        return

    # åˆ›å»ºç¯å¢ƒ
    try:
        env = DummyVecEnv([make_env(render_mode=None)])
        env = VecNormalize(env, norm_obs=True, norm_reward=True)
    except Exception as e:
        print(f"é”™è¯¯ï¼šåˆ›å»ºç¯å¢ƒå¤±è´¥ - {e}")
        return

    # ä¼˜åŒ–å­¦ä¹ å‚æ•°
    learning_rate = 3e-4  # æé«˜å­¦ä¹ ç‡
    buffer_size = 100000
    batch_size = 256
    gamma = 0.99
    tau = 0.005

    # åŠ¨æ€ç›®æ ‡ç†µ
    temp_env = make_env()()
    action_dim = temp_env.action_space.shape[0]
    temp_env.close()
    target_entropy = -float(action_dim)

    # åˆ›å»ºæ¨¡å‹ - ä½¿ç”¨æ›´æ¿€è¿›çš„å‚æ•°
    model = SAC(
        "MlpPolicy",
        env,
        device='cuda',
        verbose=1,
        learning_rate=learning_rate,
        buffer_size=buffer_size,
        batch_size=batch_size,
        gamma=gamma,
        tau=tau,
        ent_coef='auto',
        target_entropy=target_entropy,
        tensorboard_log="./logs/",
        policy_kwargs={
            "net_arch": {"pi": [512, 512], "qf": [512, 512]}  # æ›´å¤§çš„ç½‘ç»œ
        },
        target_update_interval=1,
    )

    # å›è°ƒå‡½æ•°
    checkpoint_callback = CheckpointCallback(
        save_freq=10000,
        save_path="./models/",
        name_prefix="sac_7dof_logs"
    )

    # åˆ›å»ºè¯„ä¼°ç¯å¢ƒ
    eval_env = DummyVecEnv([make_env(render_mode=None)])
    eval_env = VecNormalize(eval_env, norm_obs=True, norm_reward=True)
    eval_env.obs_rms = env.obs_rms
    eval_env.ret_rms = env.ret_rms

    eval_callback = EvalCallback(
        eval_env,
        best_model_save_path="./models/best/",
        log_path="./logs/",
        eval_freq=5000,
        deterministic=True,
        render=False
    )

    # è®­ç»ƒæ—¥å¿—è®°å½•å™¨
    training_logger = TrainingLogger()

    # å¼€å§‹è®­ç»ƒ
    print("ğŸš€ å¼€å§‹è®­ç»ƒ...")
    print(f"ğŸ“Š æ€»æ—¶é—´æ­¥: {config['total_timesteps']}")
    print(f"ğŸ¯ å­¦ä¹ ç‡: {learning_rate}")
    print(f"ğŸ“ˆ TensorBoardæ—¥å¿—: ./logs/")
    print(f"ğŸ’¾ æ¨¡å‹ä¿å­˜: ./models/")
    print("=" * 50)
    
    model.learn(
        total_timesteps=config['total_timesteps'],
        callback=[checkpoint_callback, eval_callback, training_logger],
        tb_log_name="sac_7dof_with_logs"
    )
    
    print("=" * 50)
    print("âœ… è®­ç»ƒå®Œæˆï¼")
    print(f"ğŸ“ˆ æ€»episodes: {training_logger.total_episodes}")
    print(f"ğŸ¯ æ€»æˆåŠŸç‡: {training_logger.success_count / max(1, training_logger.total_episodes):.2%}")
    print(f"ğŸ“Š å¹³å‡å¥–åŠ±: {np.mean(training_logger.episode_rewards):.2f}")

    # ä¿å­˜æ¨¡å‹
    model.save("./models/sac_7dof_with_logs_final")
    env.save("./models/vec_normalize_with_logs.pkl")

    # å…³é—­ç¯å¢ƒ
    env.close()
    eval_env.close()

if __name__ == "__main__":
    # ç¡®ä¿ä¿å­˜ç›®å½•å­˜åœ¨
    os.makedirs("./models/", exist_ok=True)
    os.makedirs("./logs/", exist_ok=True)
    train_with_logs()
