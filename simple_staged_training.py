#!/usr/bin/env python3
"""
ç®€åŒ–çš„åˆ†é˜¶æ®µè®­ç»ƒè„šæœ¬
"""

import os
import yaml
from stable_baselines3 import SAC
from stable_baselines3.common.callbacks import CheckpointCallback, BaseCallback
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize
from envs.seven_dof_arm import SevenDOFArmEnv

class EpisodeLoggerCallback(BaseCallback):
    def _on_step(self) -> bool:
        infos = self.locals.get('infos', [])
        for info in infos:
            if 'episode' in info:
                ep_r = info['episode']['r']
                ep_l = info['episode']['l']
                print(f"[Episode] reward: {ep_r:.2f}, length: {ep_l}")
        return True

def make_env(render_mode=None):
    def _init():
        env = SevenDOFArmEnv(render_mode=render_mode, model_path='franka/panda.xml')
        env = Monitor(env)
        return env
    return _init

def train_stage(stage_name, timesteps, success_mode="distance", threshold=0.5):
    """åˆ†é˜¶æ®µè®­ç»ƒ"""
    print(f"\nğŸš€ å¼€å§‹ {stage_name} è®­ç»ƒ...")
    print(f"æˆåŠŸæ¡ä»¶: {success_mode}, é˜ˆå€¼: {threshold}")
    
    # åˆ›å»ºç¯å¢ƒ
    env = DummyVecEnv([make_env(render_mode=None)])
    env = VecNormalize(env, norm_obs=True, norm_reward=False)
    
    # è®¾ç½®æˆåŠŸæ¡ä»¶
    for env_wrapper in env.envs:
        if hasattr(env_wrapper.env, 'set_success_condition'):
            env_wrapper.env.set_success_condition(success_mode, threshold)
    
    # åˆ›å»ºæ¨¡å‹
    model = SAC(
        "MlpPolicy",
        env,
        device='cuda',
        verbose=2,
        learning_rate=1e-4,
        buffer_size=200000,
        batch_size=512,
        gamma=0.995,
        tau=0.01,
        ent_coef='auto',
        tensorboard_log="./logs/",
        policy_kwargs={"net_arch": [512, 512, 256]},
        target_update_interval=1,
    )
    
    # å›è°ƒå‡½æ•°
    callback = EpisodeLoggerCallback()
    callback._training_env = env
    callback._total_timesteps = timesteps
    
    checkpoint_callback = CheckpointCallback(
        save_freq=timesteps // 4,
        save_path=f"./models/{stage_name}/",
        name_prefix="sac_7dof"
    )
    
    # å¼€å§‹è®­ç»ƒ
    model.learn(
        total_timesteps=timesteps,
        callback=[checkpoint_callback, callback],
        tb_log_name=stage_name,
        log_interval=100
    )
    
    # ä¿å­˜æ¨¡å‹
    model.save(f"./models/{stage_name}_final")
    env.save(f"./models/{stage_name}_vec_normalize.pkl")
    
    env.close()
    print(f"âœ… {stage_name} è®­ç»ƒå®Œæˆï¼")
    
    return model

def main():
    """ä¸»è®­ç»ƒæµç¨‹"""
    os.makedirs("./models/", exist_ok=True)
    os.makedirs("./logs/", exist_ok=True)
    
    print("ğŸ¯ å¼€å§‹åˆ†é˜¶æ®µæŠ“å–è®­ç»ƒ...")
    
    # é˜¶æ®µ1ï¼šæ¥è¿‘è®­ç»ƒ (è·ç¦»<0.5m)
    print("\n" + "="*50)
    print("é˜¶æ®µ1ï¼šæ¥è¿‘è®­ç»ƒ")
    print("ç›®æ ‡ï¼šå­¦ä¼šæ¥è¿‘ç›®æ ‡ç‰©ä½“")
    print("æˆåŠŸæ¡ä»¶ï¼šè·ç¦» < 0.5m")
    print("="*50)
    train_stage("stage1_approach", timesteps=20000, success_mode="distance", threshold=0.5)
    
    # é˜¶æ®µ2ï¼šç²¾ç¡®æ¥è¿‘ (è·ç¦»<0.3m)
    print("\n" + "="*50)
    print("é˜¶æ®µ2ï¼šç²¾ç¡®æ¥è¿‘")
    print("ç›®æ ‡ï¼šæ›´ç²¾ç¡®åœ°æ¥è¿‘ç›®æ ‡")
    print("æˆåŠŸæ¡ä»¶ï¼šè·ç¦» < 0.3m")
    print("="*50)
    train_stage("stage2_precise", timesteps=20000, success_mode="distance", threshold=0.3)
    
    # é˜¶æ®µ3ï¼šæ¥è§¦è®­ç»ƒ (å•æŒ‡æ¥è§¦)
    print("\n" + "="*50)
    print("é˜¶æ®µ3ï¼šæ¥è§¦è®­ç»ƒ")
    print("ç›®æ ‡ï¼šå­¦ä¼šæ¥è§¦ç›®æ ‡ç‰©ä½“")
    print("æˆåŠŸæ¡ä»¶ï¼šå•æŒ‡æ¥è§¦")
    print("="*50)
    train_stage("stage3_contact", timesteps=20000, success_mode="contact")
    
    # é˜¶æ®µ4ï¼šæŠ“å–è®­ç»ƒ (åŒæŒ‡æ¥è§¦)
    print("\n" + "="*50)
    print("é˜¶æ®µ4ï¼šæŠ“å–è®­ç»ƒ")
    print("ç›®æ ‡ï¼šå­¦ä¼šæŠ“å–ç›®æ ‡ç‰©ä½“")
    print("æˆåŠŸæ¡ä»¶ï¼šåŒæŒ‡æ¥è§¦")
    print("="*50)
    train_stage("stage4_grasp", timesteps=20000, success_mode="grasp")
    
    print("\nğŸ‰ åˆ†é˜¶æ®µè®­ç»ƒå®Œæˆï¼")

if __name__ == "__main__":
    main()
