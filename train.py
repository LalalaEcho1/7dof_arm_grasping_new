import os
import time
import yaml
import argparse
import numpy as np
import torch
import gymnasium as gym  # ç¡®ä¿å¯¼å…¥ gymnasium
import multiprocessing as mp

# å¼•å…¥ TensorBoard å¿…è¦çš„åº“
from torch.utils.tensorboard import SummaryWriter

try:
    mp.set_start_method('spawn', force=True)
except RuntimeError:
    pass

torch.set_num_threads(1)
os.environ['OMP_NUM_THREADS'] = '1'

from stable_baselines3 import SAC
from stable_baselines3.common.callbacks import CheckpointCallback, BaseCallback, CallbackList
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.vec_env import SubprocVecEnv, VecNormalize, DummyVecEnv
from stable_baselines3.common.utils import set_random_seed

# å‡è®¾ä½ çš„ç¯å¢ƒæ–‡ä»¶åœ¨è¿™é‡Œ
from envs.seven_dof_arm import SevenDOFArmEnv


# ==========================================
# 1. æ–°å¢ï¼šå…¨èƒ½ TensorBoard å¯è§†åŒ–å›è°ƒ
# ==========================================
class TensorboardCallback(BaseCallback):
    """
    é›†æˆè§†é¢‘å½•åˆ¶ã€Qå€¼ç›´æ–¹å›¾ã€åŠ¨ä½œåˆ†å¸ƒç›‘æ§çš„å›è°ƒå‡½æ•°
    """
    def __init__(self, make_env_fn, video_freq=50000, debug_freq=2000, verbose=0):
        super().__init__(verbose)
        self.video_freq = video_freq
        self.debug_freq = debug_freq
        self.make_env_fn = make_env_fn
        self.eval_env = None  # ç”¨äºå½•åˆ¶è§†é¢‘çš„ç‹¬ç«‹ç¯å¢ƒ

    def _init_callback(self) -> None:
        # åˆå§‹åŒ–ä¸€ä¸ªç”¨äºè¯„ä¼°å’Œå½•åˆ¶çš„ç‹¬ç«‹ç¯å¢ƒ (DummyVecEnv)
        # æ³¨æ„ï¼šè¿™é‡Œå¿…é¡»ä½¿ç”¨ render_mode='rgb_array'
        self.eval_env = DummyVecEnv([self.make_env_fn(render_mode='rgb_array', render_every=1)])
        
        # å¦‚æœè®­ç»ƒç¯å¢ƒä½¿ç”¨äº† VecNormalizeï¼Œè¯„ä¼°ç¯å¢ƒä¹Ÿå¿…é¡»åŒ…è£¹ VecNormalize
        # ä½†è¯„ä¼°æ—¶ä¸éœ€è¦æ›´æ–°ç»Ÿè®¡æ•°æ® (training=False)ï¼Œä¹Ÿä¸éœ€è¦å½’ä¸€åŒ–å¥–åŠ± (norm_reward=False)
        if isinstance(self.training_env, VecNormalize):
            self.eval_env = VecNormalize(self.eval_env, norm_obs=True, norm_reward=False, clip_obs=10.0, training=False)

    def _on_step(self) -> bool:
        # --- A. è®°å½•ç›´æ–¹å›¾ (Qå€¼, åŠ¨ä½œ, æƒé‡) ---
        if self.num_timesteps % self.debug_freq == 0:
            self._log_histograms()

        # --- B. å½•åˆ¶è§†é¢‘ ---
        if self.num_timesteps % self.video_freq == 0:
            self._record_video()

        return True

    def _log_histograms(self):
        # è·å–å½“å‰çš„ logger (SB3 çš„ logger å¹¶æ²¡æœ‰ç›´æ¥æš´éœ² add_histogramï¼Œæˆ‘ä»¬éœ€è¦æ‹¿åˆ°åº•å±‚çš„ writer)
        # æ³¨æ„ï¼šè¿™é‡Œé€šè¿‡ä¸€ç§ hack çš„æ–¹å¼è·å– TensorBoard writer
        tb_writer = None
        for format in self.logger.output_formats:
            if format.__class__.__name__ == 'TensorBoardOutputFormat':
                tb_writer = format.writer
                break
        
        if tb_writer is not None:
            with torch.no_grad():
                # 1. åŠ¨ä½œåˆ†å¸ƒ
                # è·å–å½“å‰ batch çš„åŠ¨ä½œ (æ¥è‡ª replay buffer æˆ–å½“å‰ step)
                # ä¸ºäº†ç®€å•ï¼Œæˆ‘ä»¬ç›´æ¥ç”¨å½“å‰ç­–ç•¥å¯¹å½“å‰ observation é¢„æµ‹ä¸€æ¬¡
                obs = self.locals['new_obs'] # è·å–å½“å‰æ­¥çš„ observation
                if isinstance(obs, dict): # å¤„ç† Dict spaces
                    pass 
                else:
                    obs_tensor = torch.as_tensor(obs).to(self.model.device)
                    actions, _ = self.model.actor(obs_tensor) # é¢„æµ‹åŠ¨ä½œ
                    
                    tb_writer.add_histogram('Debug/Action_Distribution', actions, self.num_timesteps)

                    # 2. Q å€¼åˆ†å¸ƒ (æ£€æŸ¥ Critic æ˜¯å¦è¿‡ä¼°è®¡)
                    # ä½¿ç”¨ Critic ç½‘ç»œè¯„ä¼°è¿™äº›åŠ¨ä½œ
                    q1_values, q2_values = self.model.critic(obs_tensor, actions)
                    tb_writer.add_histogram('Debug/Q_Values', q1_values, self.num_timesteps)

                    # 3. è®°å½• Critic Loss (è™½ç„¶ SB3 è‡ªå¸¦ï¼Œä½†è¿™é‡Œå¯ä»¥åšæ›´ç»†è‡´çš„æ£€æŸ¥)
                    # (SB3 é»˜è®¤å·²ç»è®°å½•äº† train/critic_lossï¼Œè¿™é‡Œä¸åšé‡å¤)

    def _record_video(self):
        if self.verbose > 0:
            print(f"ğŸ¥ [TensorboardCallback] æ­£åœ¨å½•åˆ¶è§†é¢‘ @ Step {self.num_timesteps}...")

        # >>> å…³é”®æ­¥éª¤ï¼šåŒæ­¥ VecNormalize çš„ç»Ÿè®¡æ•°æ® <<<
        # å¦‚æœè®­ç»ƒç¯å¢ƒæ˜¯å½’ä¸€åŒ–çš„ï¼Œè¯„ä¼°ç¯å¢ƒå¿…é¡»æ‹¥æœ‰ç›¸åŒçš„å‡å€¼å’Œæ–¹å·®ï¼Œå¦åˆ™æœºå™¨äººåƒæ˜¯åœ¨"ç›²äººæ‘¸è±¡"
        if isinstance(self.training_env, VecNormalize):
            # ä»å¹¶è¡Œç¯å¢ƒåŒæ­¥ obs_rms åˆ°è¯„ä¼°ç¯å¢ƒ
            self.eval_env.obs_rms = self.training_env.obs_rms

        screens = []
        obs = self.eval_env.reset()
        done = False
        
        # è¿è¡Œä¸€ä¸ªå®Œæ•´å›åˆ
        while True:
            # æ¸²æŸ“å¸§ (H, W, C)
            # å¯¹äº DummyVecEnvï¼Œrender è¿”å›çš„æ˜¯ list of arrays
            img = self.eval_env.render() 
            if isinstance(img, list): img = img[0] # å–ç¬¬ä¸€ä¸ªç¯å¢ƒçš„å›¾åƒ
            screens.append(img)

            # ç¡®å®šæ€§åŠ¨ä½œé€‰æ‹©
            action, _ = self.model.predict(obs, deterministic=True)
            obs, reward, done, infos = self.eval_env.step(action)
            
            if done[0]: # DummyVecEnv è¿”å›çš„æ˜¯ done æ•°ç»„
                break

        # è½¬æ¢æ ¼å¼ä¸º TensorBoard éœ€è¦çš„ (N, T, C, H, W)
        # screens: (T, H, W, C) -> (1, T, C, H, W)
        if len(screens) > 0:
            screens_np = np.array(screens)
            # è½¬æ¢ä¸º Tensor: [T, H, W, C] -> [T, C, H, W]
            video_tensor = torch.from_numpy(screens_np).permute(0, 3, 1, 2).unsqueeze(0)
            
            # è·å– writer å¹¶å†™å…¥
            for format in self.logger.output_formats:
                if format.__class__.__name__ == 'TensorBoardOutputFormat':
                    format.writer.add_video('Rollout/Video', video_tensor, self.num_timesteps, fps=30)
                    format.writer.flush()
                    break

    def _on_training_end(self) -> None:
        if self.eval_env is not None:
            self.eval_env.close()


# ==========================================
# 2. åŸæœ‰çš„è¯¾ç¨‹å­¦ä¹ å›è°ƒ (ä¿æŒä¸å˜)
# ==========================================
class CurriculumCallback(BaseCallback):
    """
    åŸºäºè®­ç»ƒè¿›åº¦çš„è¯¾ç¨‹å­¦ä¹ å›è°ƒ
    """
    def __init__(self, total_timesteps, update_every_steps=500, verbose=1):
        super().__init__(verbose)
        self.total_timesteps = float(total_timesteps)
        self.update_every_steps = int(update_every_steps)

    def _on_step(self) -> bool:
        if self.num_timesteps % self.update_every_steps != 0:
            return True

        progress = float(self.num_timesteps) / max(1.0, self.total_timesteps)

        if self.verbose >= 2:
            print(f"\n[Curriculum] æ­¥æ•°: {self.num_timesteps}, è¿›åº¦: {progress:.2%}")

        try:
            self.training_env.env_method("set_training_progress", progress)
        except Exception as e:
            print(f"[CurriculumCallback] âš ï¸ æ›´æ–°ç¯å¢ƒå‚æ•°å¤±è´¥: {e}")

        return True


class EpisodeLoggerCallback(BaseCallback):
    """
    Episodeæ—¥å¿—å›è°ƒ - ä»…ç”¨äºæ§åˆ¶å°è¾“å‡º
    """
    def __init__(self, verbose=0):
        super().__init__(verbose)

    def _on_step(self) -> bool:
        infos = self.locals.get('infos', [])
        for info in infos:
            if 'episode' in info:
                ep_r = info['episode'].get('r', 0.0)
                ep_l = info['episode'].get('l', 0)
                if self.verbose >= 1:
                    print(f"  > Ep End: R={ep_r:.1f}, L={ep_l}")
        return True


# ==========================================
# 3. ç¯å¢ƒå·¥å‚ä¸è®­ç»ƒé€»è¾‘
# ==========================================

def make_env_builder(render_mode=None, render_every=1000):
    """å·¥å‚å‡½æ•°ï¼Œè¿”å›ä¸€ä¸ªåˆ›å»ºç¯å¢ƒçš„å‡½æ•°"""
    def _init():
        # æ³¨æ„ï¼šrender_mode å¿…é¡»åœ¨è¿™é‡Œä¼ é€’ç»™ SevenDOFArmEnv
        env = SevenDOFArmEnv(render_mode=render_mode, model_path="franka/panda.xml", render_every=render_every)
        env = Monitor(env)
        return env
    return _init

def train(config_path="config.yaml", visualize_after=False):
    with open(config_path, "r") as f:
        config = yaml.safe_load(f)

    seed = int(config.get("seed", 42))
    set_random_seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)

    print("ğŸ” æµ‹è¯•ç¯å¢ƒ...")
    # æµ‹è¯•ç¯å¢ƒåˆ›å»º
    test_env = make_env_builder(render_mode='rgb_array')()
    test_env.reset()
    test_env.close()
    del test_env
    print("âœ… ç¯å¢ƒæµ‹è¯•é€šè¿‡ï¼\n")

    # åˆ›å»ºå¹¶è¡Œè®­ç»ƒç¯å¢ƒ
    num_cpu = 4 # å»ºè®®æ ¹æ® CPU æ ¸å¿ƒæ•°è°ƒæ•´ï¼Œ2 å¯èƒ½å¤ªå°‘
    # æ³¨æ„ï¼šè®­ç»ƒç¯å¢ƒé€šå¸¸ä¸éœ€è¦ render_mode (ä¸ºäº†é€Ÿåº¦)ï¼Œé™¤éä¸ºäº†è°ƒè¯•
    env = SubprocVecEnv([make_env_builder(render_mode=None) for _ in range(num_cpu)], start_method='spawn')
    env = VecNormalize(env, norm_obs=True, norm_reward=False, clip_obs=10.0, training=True)

    print(f"âœ… ç¯å¢ƒé…ç½®: SubprocVecEnv(n={num_cpu}), VecNormalize=True\n")

    total_timesteps = int(config.get("train_params", {}).get("total_timesteps", 2000000))
    learning_rate = float(config["model_params"]["learning_rate"])

    print(f"ğŸš€ å¼€å§‹è®­ç»ƒ | è¿›åº¦é©±åŠ¨è¯¾ç¨‹å­¦ä¹ ")

    policy_kwargs = config.get("policy_kwargs", {"net_arch": [256, 256]})
    ent_coef_conf = config["model_params"].get("ent_coef", "auto")
    
    # å¤„ç† ent_coef ç±»å‹è½¬æ¢
    if isinstance(ent_coef_conf, str) and ent_coef_conf != "auto":
        try:
            ent_coef_value = float(ent_coef_conf)
        except ValueError:
            ent_coef_value = "auto"
    else:
        ent_coef_value = ent_coef_conf

    model = SAC(
        "MlpPolicy",
        env,
        device="cuda" if torch.cuda.is_available() else "cpu",
        verbose=1,
        learning_rate=learning_rate,
        buffer_size=int(config["model_params"]["buffer_size"]),
        batch_size=int(config["model_params"]["batch_size"]),
        gamma=float(config["model_params"].get("gamma", 0.99)),
        tau=float(config["model_params"].get("tau", 0.01)),
        ent_coef=ent_coef_value,
        tensorboard_log="./logs/",
        policy_kwargs=policy_kwargs,
        target_update_interval=int(config.get("target_update_interval", 1)),
    )

    # å›è°ƒé…ç½®
    curriculum_ctrl = config.get("curriculum_control", {})
    update_every = int(curriculum_ctrl.get("update_every_steps", 500))

    # --- é…ç½®å¯è§†åŒ–å›è°ƒ ---
    # video_freq: æ¯”å¦‚æ¯ 50,000 æ­¥å½•åˆ¶ä¸€æ¬¡
    # debug_freq: æ¯ 2,000 æ­¥è®°å½•ä¸€æ¬¡ Q å€¼åˆ†å¸ƒ
    tensorboard_cb = TensorboardCallback(
        make_env_fn=make_env_builder, 
        video_freq=50000, 
        debug_freq=2000,
        verbose=1
    )

    callbacks = [
        CheckpointCallback(save_freq=int(config["callbacks"]["checkpoint_freq"]), save_path="./models/", name_prefix="sac_7dof"),
        CurriculumCallback(total_timesteps, update_every_steps=update_every, verbose=1),
        EpisodeLoggerCallback(verbose=1),
        tensorboard_cb  # <--- æ·»åŠ æ–°çš„å›è°ƒ
    ]

    model.learn(
        total_timesteps=total_timesteps,
        callback=CallbackList(callbacks),
        tb_log_name="sac_7dof",
        log_interval=100,
    )

    os.makedirs("./models/", exist_ok=True)
    model.save("./models/sac_7dof_final")
    try:
        env.save("./models/vec_normalize.pkl")
    except Exception:
        pass
    env.close()

    print("\nâœ… è®­ç»ƒå®Œæˆï¼")

    if visualize_after:
        _visualize_model("./models/sac_7dof_final.zip")


def _visualize_model(model_path, episodes=3):
    print(f"\nğŸ® å¯è§†åŒ–æ¼”ç¤º: {model_path}")
    
    # æ¼”ç¤ºç¯å¢ƒå¿…é¡»å¼€å¯ render_mode='human'
    vis_env = make_env_builder(render_mode='human', render_every=1)()
    
    # æ­£ç¡®çš„åŠ è½½æ–¹å¼ï¼šå¿…é¡»åŠ è½½è®­ç»ƒæ—¶çš„ VecNormalize ç»Ÿè®¡æ•°æ®
    # å¦åˆ™æ¼”ç¤ºæ—¶çš„åŠ¨ä½œä¼šéå¸¸é¬¼ç•œ
    if os.path.exists("./models/vec_normalize.pkl"):
        print("   åŠ è½½ VecNormalize ç»Ÿè®¡æ•°æ®...")
        # å¿…é¡»ç”¨ DummyVecEnv åŒ…è£¹æ‰èƒ½ä½¿ç”¨ VecNormalize.load
        vis_env = DummyVecEnv([lambda: vis_env]) 
        vis_env = VecNormalize.load("./models/vec_normalize.pkl", vis_env)
        vis_env.training = False # æµ‹è¯•æ¨¡å¼ï¼Œä¸æ›´æ–°ç»Ÿè®¡
        vis_env.norm_reward = False
    else:
        print("âš ï¸ æœªæ‰¾åˆ° vec_normalize.pklï¼Œä½¿ç”¨åŸå§‹è§‚æµ‹å€¼ (å¯èƒ½å¯¼è‡´æ¼”ç¤ºæ•ˆæœæå·®)")

    model = SAC.load(model_path)

    for ep in range(episodes):
        obs = vis_env.reset() # VecNormalize åŒ…è£¹å reset ä¸éœ€è¦ _
        done = False
        total_reward = 0
        while not done:
            action, _ = model.predict(obs, deterministic=True)
            obs, reward, done, infos = vis_env.step(action)
            total_reward += reward
            time.sleep(0.02)
        print(f"æ¼”ç¤º Ep {ep + 1}: Reward={total_reward[0]:.2f}") # VecEnv è¿”å›æ•°ç»„
    vis_env.close()


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--config", type=str, default="config.yaml")
    parser.add_argument("--visualize-after", action="store_true")
    args = parser.parse_args()

    os.makedirs("./models", exist_ok=True)
    os.makedirs("./logs", exist_ok=True)

    train(config_path=args.config, visualize_after=args.visualize_after)
