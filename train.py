import os
import time
import yaml
import argparse
import numpy as np
import torch
import multiprocessing as mp

try:
    mp.set_start_method('spawn', force=True)
except RuntimeError:
    pass

torch.set_num_threads(1)
os.environ['OMP_NUM_THREADS'] = '1'

from stable_baselines3 import SAC
from stable_baselines3.common.callbacks import CheckpointCallback, BaseCallback, CallbackList
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.vec_env import SubprocVecEnv, VecNormalize
from stable_baselines3.common.utils import set_random_seed

from envs.seven_dof_arm import SevenDOFArmEnv


class CurriculumCallback(BaseCallback):
    """
    åŸºäºè®­ç»ƒè¿›åº¦çš„è¯¾ç¨‹å­¦ä¹ å›è°ƒ
    [ä¿®å¤] ä½¿ç”¨ env_method è·¨è¿›ç¨‹æ›´æ–°ç¯å¢ƒå‚æ•°
    """

    def __init__(self, total_timesteps, update_every_steps=500, verbose=1):
        super().__init__(verbose)
        self.total_timesteps = float(total_timesteps)
        self.update_every_steps = int(update_every_steps)

    def _on_step(self) -> bool:
        if self.num_timesteps % self.update_every_steps != 0:
            return True

        # è®¡ç®—è®­ç»ƒè¿›åº¦
        progress = float(self.num_timesteps) / max(1.0, self.total_timesteps)

        if self.verbose >= 2:
            print(f"\n[Curriculum] æ­¥æ•°: {self.num_timesteps}, è¿›åº¦: {progress:.2%}")

        # ğŸš€ [å…³é”®ä¿®å¤] ä½¿ç”¨ env_method å¹¿æ’­ç»™æ‰€æœ‰å­è¿›ç¨‹
        # self.training_env æ˜¯ä¸€ä¸ª VecEnv (å¯èƒ½æ˜¯ VecNormalize åŒ…è£¹çš„ SubprocVecEnv)
        # env_method ä¼šè‡ªåŠ¨ç©¿é€ Wrappers å¹¶é€šè¿‡ Pipe å‘é€ç»™å­è¿›ç¨‹
        try:
            self.training_env.env_method("set_training_progress", progress)
        except Exception as e:
            print(f"[CurriculumCallback] âš ï¸ æ›´æ–°ç¯å¢ƒå‚æ•°å¤±è´¥: {e}")

        return True


class EpisodeLoggerCallback(BaseCallback):
    """
    Episodeæ—¥å¿—å›è°ƒ - ä»…ç”¨äºæ§åˆ¶å°è¾“å‡º
    ç¯å¢ƒå†…éƒ¨çš„ç»Ÿè®¡ç°åœ¨ç”±ç¯å¢ƒè‡ªå·±ç»´æŠ¤
    """

    def __init__(self, verbose=0):
        super().__init__(verbose)

    def _on_step(self) -> bool:
        infos = self.locals.get('infos', [])
        for info in infos:
            if 'episode' in info:
                ep_r = info['episode'].get('r', 0.0)
                ep_l = info['episode'].get('l', 0)
                if self.verbose >= 1:
                    # ç®€å•çš„å•è¡Œæ—¥å¿—
                    print(f"  > Ep End: R={ep_r:.1f}, L={ep_l}")
        return True


def make_env(render_mode=None, render_every=1000):
    def _init():
        env = SevenDOFArmEnv(render_mode=render_mode, model_path="franka/panda.xml", render_every=render_every)
        env = Monitor(env)
        return env

    return _init


def train(config_path="config.yaml", visualize_after=False):
    with open(config_path, "r") as f:
        config = yaml.safe_load(f)

    seed = int(config.get("seed", 42))
    set_random_seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)

    print("ğŸ” æµ‹è¯•ç¯å¢ƒ...")
    test_env = make_env(render_mode='None', render_every=1000)()
    test_env.reset()
    test_env.close()
    del test_env
    print("âœ… ç¯å¢ƒæµ‹è¯•é€šè¿‡ï¼\n")

    # åˆ›å»ºå¹¶è¡Œè®­ç»ƒç¯å¢ƒ
    num_cpu = 2
    env = SubprocVecEnv([make_env(render_mode='None', render_every=1000) for _ in range(num_cpu)], start_method='spawn')
    env = VecNormalize(env, norm_obs=True, norm_reward=False, clip_obs=10.0, training=True)

    print(f"âœ… ç¯å¢ƒé…ç½®: SubprocVecEnv(n={num_cpu}), VecNormalize=True\n")

    total_timesteps = int(config.get("train_params", {}).get("total_timesteps", 2000000))
    learning_rate = float(config["model_params"]["learning_rate"])

    print(f"ğŸš€ å¼€å§‹è®­ç»ƒ | è¿›åº¦é©±åŠ¨è¯¾ç¨‹å­¦ä¹ ")
    print(f"   åˆ‡æ¢ç‚¹: 25% -> 50% -> 75%\n")

    policy_kwargs = config.get("policy_kwargs", {"net_arch": [256, 256]})
    ent_coef_conf = config["model_params"].get("ent_coef", "auto")
    ent_coef_value = "auto" if isinstance(ent_coef_conf, str) else float(ent_coef_conf)

    model = SAC(
        "MlpPolicy",
        env,
        device="cuda" if torch.cuda.is_available() else "cpu",
        verbose=1,
        learning_rate=learning_rate,
        buffer_size=int(config["model_params"]["buffer_size"]),
        batch_size=int(config["model_params"]["batch_size"]),
        gamma=float(config["model_params"].get("gamma", 0.99)),
        tau=float(config["model_params"].get("tau", 0.01)),
        ent_coef=ent_coef_value,
        tensorboard_log="./logs/",
        policy_kwargs=policy_kwargs,
        target_update_interval=int(config.get("target_update_interval", 1)),
    )

    # å›è°ƒé…ç½®
    curriculum_ctrl = config.get("curriculum_control", {})
    update_every = int(curriculum_ctrl.get("update_every_steps", 500))

    # æ³¨æ„é¡ºåºï¼šCheckPoint -> Curriculum -> Logger
    callbacks = [
        CheckpointCallback(save_freq=int(config["callbacks"]["checkpoint_freq"]), save_path="./models/",
                           name_prefix="sac_7dof"),
        CurriculumCallback(total_timesteps, update_every_steps=update_every, verbose=1),
        EpisodeLoggerCallback(verbose=1)
    ]

    model.learn(
        total_timesteps=total_timesteps,
        callback=CallbackList(callbacks),
        tb_log_name="sac_7dof",
        log_interval=100,
    )

    os.makedirs("./models/", exist_ok=True)
    model.save("./models/sac_7dof_final")
    try:
        env.save("./models/vec_normalize.pkl")
    except Exception:
        pass
    env.close()

    print("\nâœ… è®­ç»ƒå®Œæˆï¼")

    if visualize_after:
        _visualize_model("./models/sac_7dof_final.zip")


def _visualize_model(model_path, episodes=3):
    print(f"\nğŸ® å¯è§†åŒ–: {model_path}")
    # å¯è§†åŒ–ä½¿ç”¨å•ä¸ªDummyç¯å¢ƒ
    vis_env = make_env(render_mode='human', render_every=1)()
    # å¦‚æœç”¨äº†VecNormalizeï¼Œè¿™é‡Œå…¶å®åº”è¯¥åŠ è½½ç»Ÿè®¡æ•°æ®ï¼Œå¦åˆ™åŠ¨ä½œä¼šå¾ˆå¥‡æ€ª
    # ä½†ä¸ºäº†ç®€å•æ¼”ç¤ºï¼Œè¿™é‡Œç›´æ¥è¿è¡ŒåŸå§‹ç¯å¢ƒï¼Œæ•ˆæœå¯èƒ½ä¸€èˆ¬

    # æ›´ä¸¥è°¨çš„åšæ³•æ˜¯åŒ…è£¹ DummyVecEnv å¹¶åŠ è½½ pkl
    # from stable_baselines3.common.vec_env import DummyVecEnv
    # vis_env = DummyVecEnv([lambda: make_env(render_mode='human', render_every=1)()])
    # vis_env = VecNormalize.load("./models/vec_normalize.pkl", vis_env)
    # vis_env.training = False
    # vis_env.norm_reward = False

    model = SAC.load(model_path)  # åŠ è½½æ¨¡å‹

    for ep in range(episodes):
        obs, _ = vis_env.reset()
        done = False
        total_reward = 0
        while not done:
            # è‹¥æœªä½¿ç”¨VecNormalizeåŠ è½½ï¼Œè¿™é‡Œçš„obsèŒƒå›´å¯èƒ½å’Œè®­ç»ƒæ—¶ä¸ä¸€è‡´
            action, _ = model.predict(obs, deterministic=True)
            obs, reward, terminated, truncated, _ = vis_env.step(action)
            total_reward += reward
            done = terminated or truncated
            time.sleep(0.02)
        print(f"æ¼”ç¤º Ep {ep + 1}: Reward={total_reward:.2f}")
    vis_env.close()


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--config", type=str, default="config.yaml")
    parser.add_argument("--visualize-after", action="store_true")
    args = parser.parse_args()

    os.makedirs("./models", exist_ok=True)
    os.makedirs("./logs", exist_ok=True)

    train(config_path=args.config, visualize_after=args.visualize_after)
